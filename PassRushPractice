import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
import statsmodels.api as sm

## Load last year's NFL data
df=pd.read_csv(r"C:\Users\quint\OneDrive\Desktop\Python\PassRusherProfile\Data\NFLData2024.csv")

## create a dictionary of important stats that are relevant to a pass rusher then set the unique identifier as the player_id for adequate tracking over various datasets
df_filter=df[["player", "position", "player_id","player_game_count", "assists", "franchise_id", "hits", "hurries", "misses", "pass_rush_percent", "pass_rush_snaps", "pass_snaps", "pressures", "prp", "sacks", "stops", "tackles"]]
df_filter.set_index("player_id", inplace=True)
#removed all corners from the dataset
df_filter=df_filter[df_filter["position"]!= "CB"]
#used to determine the pass rush percentage cutoff to filter out players who are not primarily pass rushers (Not active in relevant pythons runs, therefore added as a comment)
# what is the proper way to host code thats no longer useful, like the histogram?
'''
plt.hist(df_filter["pass_rush_percent"], bins=20, edgecolor="black")
plt.xlabel("Pass Rush Percentage (pass_rush_percent)")
plt.ylabel("Number of Players")
plt.title("Distribution of Pass Rush %")
'''
#determined 85 was a contextually valid cutoff after validating with the histogram
#Maybe I can find a better way to determine this cutoff in the future?
df_filtered=df_filter[df_filter["pass_rush_percent"]>=85]
print(df_filtered.head())
print(len(df_filtered)) 

#selecting only numeric features for regression analysis. Also chose to remove redundant and unimportant features (pass_snaps, player_game_count)
numeric_features = ["pass_rush_snaps","pressures","stops","hits", "hurries", "misses", "sacks", "tackles"]
df_numeric = df_filter[numeric_features]

#must validate that theres no collinearity in our features. This was accounted for with the removal of redundant features
corr_matrix = df_numeric.corr()
# Plot heatmap
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
#plt.show()

#we must set up the standardization first to ensure a sack count can be compared to a snap count, which are on different scales
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_numeric), columns=numeric_features, index=df_numeric.index)

regression_results = {}

#tackles (excluding "hurries" as input)**
X_tackles = df_scaled.drop(columns=["tackles", "hurries", "stops", "pressures"])  # Removing "hurries"
y_tackles = df_scaled["tackles"]

model_tackles = LinearRegression()
model_tackles.fit(X_tackles, y_tackles)

regression_results["tackles"] = {
    "R² Score": model_tackles.score(X_tackles, y_tackles),
    "Coefficients": dict(zip(X_tackles.columns, model_tackles.coef_))
}

#sacks, excluding pressures and stops
X_sacks = df_scaled.drop(columns=["sacks", "pressures", "stops"]) 
y_sacks = df_scaled["sacks"]

model_sacks = LinearRegression()
model_sacks.fit(X_sacks, y_sacks)

regression_results["sacks"] = {
    "R² Score": model_sacks.score(X_sacks, y_sacks),
    "Coefficients": dict(zip(X_sacks.columns, model_sacks.coef_))
}
#hurries, excluding pressures and snaps
X_hurries = df_scaled.drop(columns=["hurries", "pressures", "hits", "pass_rush_snaps"])  
y_hurries = df_scaled["hurries"]

model_hurries = LinearRegression()
model_hurries.fit(X_hurries, y_hurries)

regression_results["hurries"] = {
    "R² Score": model_hurries.score(X_hurries, y_hurries),
    "Coefficients": dict(zip(X_hurries.columns, model_hurries.coef_))
}
#misses, excluding nothing
X_misses = df_scaled.drop(columns=["misses", "tackles", "stops"])  # Removing "tackles"
y_misses = df_scaled["misses"]

model_misses = LinearRegression()
model_misses.fit(X_misses, y_misses)

regression_results["misses"] = {
    "R² Score": model_misses.score(X_misses, y_misses),
    "Coefficients": dict(zip(X_misses.columns, model_misses.coef_))
}
#hits, excluding pressures
X_hits = df_scaled.drop(columns=["hits", "pressures"]) 
y_hits = df_scaled["hits"]

model_hits = LinearRegression()
model_hits.fit(X_hits, y_hits)

regression_results["hits"] = {
    "R² Score": model_hits.score(X_hits, y_hits),
    "Coefficients": dict(zip(X_hits.columns, model_hits.coef_))
}
#print all
for stat, result in regression_results.items():
    print(f"\n==== Regression for {stat} ====")
    print(f"R² Score: {result['R² Score']:.5f}")
    print("Feature Coefficients:")
    for feature, coef in result["Coefficients"].items():
        print(f"  {feature}: {coef:.5f}")
degree = 3  
model = make_pipeline(PolynomialFeatures(degree), LinearRegression())

# Example: Fit a polynomial regression model
X = df_scaled.drop(columns=["sacks", "pressures", "stops"])  # Predicting sacks
y = df_scaled["sacks"]

model.fit(X, y)
y_pred = model.predict(X)

# Check R² after adding polynomial features
r2_poly = model.score(X, y)
print(f"Polynomial R² for Sacks: {r2_poly:.3f}")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_sacks, y_sacks, test_size=0.2, random_state=42)

# Test polynomial degrees 1 through 5
for degree in range(1, 6):
    poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
    poly_model.fit(X_train, y_train)
    
    train_r2 = poly_model.score(X_train, y_train)
    test_r2 = poly_model.score(X_test, y_test)
    
    print(f"Degree {degree} - Train R²: {train_r2:.3f}, Test R²: {test_r2:.3f}")
