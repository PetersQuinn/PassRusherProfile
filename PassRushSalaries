import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
import statsmodels.api as sm
import re

#load the nfl data we will build our regression model with
NFLStats=pd.read_csv(r"C:\Users\quint\OneDrive\Desktop\Python\PassRusherProfile\Data\NFLData2024.csv")

#load all of last years salary data (accurate as of 3/8/2025)
SalaryStats=pd.read_excel(r"C:\Users\quint\OneDrive\Desktop\Python\PassRusherProfile\Data\SalaryData2024.xlsx")

# keep only relevant data from the nflstats (ignoring LHS vs RHS data)
NFL_filter=NFLStats[["player", "team_name", "position", "player_id","player_game_count", "assists", "franchise_id", "hits", "hurries", "misses", "pass_rush_percent", "pass_rush_snaps", "pass_snaps", "pressures", "prp", "sacks", "stops", "tackles"]]

#keep only relevant dat from the salary stats (ignoring team name as this will come from NFL_Filter once merged)
Salary_filter=SalaryStats[["Player", "Age","Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed", "Free Agency"]]

#remove all corners, safeties, interiors, and linebackers from the dataset
#was later discovered far more positions existed as potential outliers (like WR, QB, HB)
#these also had to be removed as the pass rush percentage was not able to remove them
NFL_filter=NFL_filter[NFL_filter["position"]!= "CB"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "S"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "LB"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "DI"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "QB"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "WR"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "HB"]
NFL_filter=NFL_filter[NFL_filter["position"]!= "TE"]

#used chat for this function, will need to verify and validate!!!!
def standardize_name(name):
    name = name.lower().strip()  # Lowercase & remove leading/trailing spaces
    name = re.sub(r"[^\w\s]", "", name)  # Remove special characters (apostrophes, hyphens, etc.)
    name = re.sub(r"\s+", " ", name)  # Replace multiple spaces with a single space
    return name

#standardizes names to make merging more accurate
NFL_filter["player"] = NFL_filter["player"].apply(standardize_name)
Salary_filter.loc[:, "Player"] = Salary_filter["Player"].apply(standardize_name)

#unfortunately there are a large number of salaried players who didn't take snaps in the 2024-2025 regular season
#these players have to be removed from the salary data
#is there a better way to do this?
players_to_remove = [
    "frankie luvu", "kyron johnson", "cameron sample", "ryder anderson", "zach harrison",
    "deshaan dixon", "william bradleyking", "nate lynn", "jihad ward", "bj ojulari",
    "chris rumph ii", "drake jackson", "cj ravenell", "earnest brown iv", "bradley chubb",
    "malcolm koonce", "jonathan garvin", "grayson murphy", "bralen trice", "derrick mclendon ii",
    "durell nchami", "jeremiah martin", "marcus haynes", "ovie oghoufo", "josh onujiogu",
    "elliott brown", "jermaine johnson ii", "larrell murchison", "tanoh kpassagnon", "deatrich wise",
    "adedayo odeleye", "sam williams", "isaiah iton", "tyreke smith", "adetomiwa adebawore",
    "daniel grzesiak", "anthony goodlow", "justin eboigbe", "david ebuka agoha", "pat oconnor",
    "deslin alexandre", "tremon morrisbrash", "bj thompson", "chris collins", "john franklinmyers",
    "jamree kromah", "julius welschof", "kentavius street", "lj collier", "malik hamm",
    "thomas rush", "junior aho", "camron peterson", "mike morris", "viliami fehoko", "luiji vilain",
    "samson ebukam", "dashawn hand", "andrew farmer", "carl jones jr"
]

#remove players who are in the `players_to_remove` list
Salary_filter = Salary_filter[~Salary_filter["Player"].isin(players_to_remove)]

# Get unique player names from both datasets
nfl_players = set(NFL_filter["player"])
salary_players = set(Salary_filter["Player"])

'''
Had to comment out the code as it isn't necessary now, but was useful for identifying errors in data parsing.
# Find players in NFL data but NOT in Salary data
nfl_not_in_salary = nfl_players - salary_players

# Find players in Salary data but NOT in NFL data
salary_not_in_nfl = salary_players - nfl_players

print(NFL_filter.head())
print(len(NFL_filter)) 

print(Salary_filter.head())
print(len(Salary_filter)) 

# Print unmatched names
print("\nPlayers in NFL data but missing from Salary data:")
for player in nfl_not_in_salary:
    print(f"NFLFilter: {player}")

print("\nPlayers in Salary data but missing from NFL data:")
for player in salary_not_in_nfl:
    print(f"SalaryFilter: {player}")

# Count the number of mismatches for reference
print(f"\nTotal unmatched in NFL filter: {len(nfl_not_in_salary)}")
print(f"Total unmatched in Salary filter: {len(salary_not_in_nfl)}")
'''
#merge salary data into NFL stats for one concise data set for our regressions
Merged_Data = NFL_filter.merge(Salary_filter, left_on="player", right_on="Player", how="left")

#drop duplicate "Player" column after merging
Merged_Data.drop(columns=["Player"], inplace=True)

#selecting only numeric features for regression analysis. Also chose to remove redundant and unimportant features (pass_snaps, player_game_count)
numeric_features = ["player_id","player_game_count", "assists", "franchise_id", "hits", "hurries", "misses", "pass_rush_percent", "pass_rush_snaps", "pass_snaps", "pressures", "prp", "sacks", "stops", "tackles", "Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"]
Merged_numeric = Merged_Data[numeric_features]

""" Not necessary for future runs of this code
#must validate that theres no collinearity in our features. This was accounted for with the removal of redundant features
corr_matrix = Merged_numeric.corr()
# Plot heatmap
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
#plt.show()"""

#we must set up the standardization first to ensure a sack count can be compared to a snap count, which are on different scales
scaler = StandardScaler()
Merged_scaled = pd.DataFrame(scaler.fit_transform(Merged_numeric), columns=numeric_features, index=Merged_numeric.index)

#pff csv's naturally leave empty stats rather than zeroes. I have to fix this here
Merged_scaled.fillna(0, inplace=True)

features_to_remove = ["pass_snaps", "hurries", "prp", "pass_rush_snaps", "stops", "misses", "player_id"]
Merged_scaled = Merged_scaled.drop(columns=features_to_remove)

""" Linear regression was NOT valuable in this case, as the R² scores were very low. A random forest approach is far more accurate.

regression_results = {}

#Total Value Model (excluding collinearities)
X_total_value = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Avg./Year"
y_total_value = Merged_scaled["Total Value"]

model_total_value = LinearRegression()
model_total_value.fit(X_total_value, y_total_value)

regression_results["Total Value"] = {
    "R² Score": model_total_value.score(X_total_value, y_total_value),
    "Coefficients": dict(zip(X_total_value.columns, model_total_value.coef_))
}

#Avg./Year Model (excluding collinearities)
X_avg_year = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Total Value"
y_avg_year = Merged_scaled["Avg./Year"]
model_avg_year = LinearRegression()
model_avg_year.fit(X_avg_year, y_avg_year)

regression_results["Avg./Year"] = {
    "R² Score": model_avg_year.score(X_avg_year, y_avg_year),
    "Coefficients": dict(zip(X_avg_year.columns, model_avg_year.coef_))
}

#Total Guaranteed Model (excluding collinearities)
X_total_guaranteed = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Total Value"
y_total_guaranteed = Merged_scaled["Total Guaranteed"]
model_total_guaranteed = LinearRegression()
model_total_guaranteed.fit(X_total_guaranteed, y_total_guaranteed)

regression_results["Total Guaranteed"] = {
    "R² Score": model_total_guaranteed.score(X_total_guaranteed, y_total_guaranteed),
    "Coefficients": dict(zip(X_total_guaranteed.columns, model_total_guaranteed.coef_))
}

#Fully Guaranteed Model (excluding collinearities)
X_fully_guaranteed = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Total Value"
y_fully_guaranteed = Merged_scaled["Fully Guaranteed"]
model_fully_guaranteed = LinearRegression()
model_fully_guaranteed.fit(X_fully_guaranteed, y_fully_guaranteed)

regression_results["Fully Guaranteed"] = {
    "R² Score": model_fully_guaranteed.score(X_fully_guaranteed, y_fully_guaranteed),
    "Coefficients": dict(zip(X_fully_guaranteed.columns, model_fully_guaranteed.coef_))
}

# Print regression results
print("Regression Results:")
for feature, results in regression_results.items():
    print(f"\n{feature}:")
    print(f"R² Score: {results['R² Score']}")
    print("Coefficients:")
    for feature_name, coefficient in results["Coefficients"].items():
        print(f"  {feature_name}: {coefficient}")"""

#we will need a series of data sets to draw conclusions for each of the salary stats.
X_total_value = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Avg./Year"
y_total_value = Merged_scaled["Total Value"]
X_avg_year = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Total Value"
y_avg_year = Merged_scaled["Avg./Year"]
X_total_guaranteed = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Total Value"
y_total_guaranteed = Merged_scaled["Total Guaranteed"]
X_fully_guaranteed = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Removing "Total Value"
y_fully_guaranteed = Merged_scaled["Fully Guaranteed"]

"""
VIF is used to ensure there's not a large collinearity between features. If there is, the model will be inaccurate.
I utilized it to determine which features to drop, in combination with contextual understanding of each stat.
It is not necessary to utilize in each run, and only clutters the terminal.
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = Merged_scaled.drop(columns=["Total Value", "Avg./Year", "Total Guaranteed", "Fully Guaranteed"])  # Remove outputs
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_data)
"""
from sklearn.ensemble import RandomForestRegressor


#determining the random forest models and feature importance for total value
total_random_forest = RandomForestRegressor(n_estimators=100, random_state=42)
total_random_forest.fit(X_total_value, y_total_value)
print("Random Forest Total Contract Value R² Score:", total_random_forest.score(X_total_value, y_total_value))

importances = total_random_forest.feature_importances_
total_importance = dict(zip(X_total_value.columns, importances))
sorted_total_importance = sorted(total_importance.items(), key=lambda x: x[1], reverse=True)

print("Feature Importances for Total Value:")
for feature, importance in sorted_total_importance:
    print(f"{feature}: {importance:.3f}")


#determining the random forest models and feature importance for average value
avg_random_forest = RandomForestRegressor(n_estimators=100, random_state=42)
avg_random_forest.fit(X_avg_year, y_avg_year)
print("Random Forest Average Value per Year R² Score:", avg_random_forest.score(X_avg_year, y_avg_year))

ave_importances = avg_random_forest.feature_importances_
total_ave_importance = dict(zip(X_avg_year.columns, ave_importances))
sorted_total_ave_importance = sorted(total_ave_importance.items(), key=lambda x: x[1], reverse=True)

print("Feature Importances for Average Value:")
for feature, importance in sorted_total_ave_importance:
    print(f"{feature}: {importance:.3f}")


#determining the random forest models and feature importance for total guaranteed value
totalG_random_forest = RandomForestRegressor(n_estimators=100, random_state=42)
totalG_random_forest.fit(X_total_guaranteed, y_total_guaranteed)
print("Random Forest Total Guaranteed Value R² Score:", totalG_random_forest.score(X_total_guaranteed, y_total_guaranteed))

G_importances = totalG_random_forest.feature_importances_
total_G_importance = dict(zip(X_total_guaranteed.columns, G_importances))
sorted_total_G_importance = sorted(total_G_importance.items(), key=lambda x: x[1], reverse=True)

print("Feature Importances for Total Guaranteed Value:")
for feature, importance in sorted_total_G_importance:
    print(f"{feature}: {importance:.3f}")


#determining the random forest models and feature importance for fully guaranteed value
full_random_forest = RandomForestRegressor(n_estimators=100, random_state=42)
full_random_forest.fit(X_fully_guaranteed, y_fully_guaranteed)
print("Random Forest Fully Guaranteed Value R² Score:", full_random_forest.score(X_fully_guaranteed, y_fully_guaranteed))

FullG_importances = full_random_forest.feature_importances_
total_FullG_importance = dict(zip(X_fully_guaranteed.columns, FullG_importances))
sorted_total_FullG_importance = sorted(total_FullG_importance.items(), key=lambda x: x[1], reverse=True)

print("Feature Importances for Fully Guaranteed Value:")
for feature, importance in sorted_total_FullG_importance:
    print(f"{feature}: {importance:.3f}")